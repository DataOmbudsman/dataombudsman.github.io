---
title: Performance Evaluation of Anomaly Detection
date: 2024-12-12 18:35:00 +0100
categories: [blog post]
tags: [machine learning, anomaly detection, evaluation]    # TAG names should always be lowercase
description: Blog posts about unsupervised ML evaluation
---

At Balabit (now One Identity), the data science team I was part of worked on unsupervised anomaly detection methods. We quickly noticed that performance evaluation—an essential aspect of any ML project—was far less standardized in this area compared to supervised learning, with fewer practical resources available. To address this, I documented our experiences and the solutions we developed. The post resonated with others facing similar challenges, leading to follow-up feedback and questions. I’m also pleased to have introduced a metric I designed (Part 3) and shared a unique aspect of ROC-AUC (Part 4) that I haven’t seen discussed elsewhere.

Here's the series of posts:
1. [How to Evaluate Unsupervised Anomaly Detection for User Behavior Analytics](https://medium.com/balabit-unsupervised/how-to-evaluate-unsupervised-anomaly-detection-for-user-behavior-analytics-88f3d5de2018)
2. [Performance Metrics for Anomaly Detection Algorithms in Security Analytics](https://medium.com/balabit-unsupervised/performance-metrics-for-anomaly-detection-algorithms-in-security-analytics-f3e509efd21d)
3. [Novel Performance Metrics for Anomaly Detection Algorithms](https://medium.com/balabit-unsupervised/novel-performance-metrics-for-anomaly-detection-algorithms-35911673ac94)
4. [A New Look at an Old Friend - How to visualize and generalize ROC-AUC](https://medium.com/balabit-unsupervised/a-new-look-at-an-old-friend-c1936841215f)
